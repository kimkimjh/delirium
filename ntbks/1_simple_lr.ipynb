{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "class LogisticRegression(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(config[\"learning_rate\"])\n",
    "\n",
    "        self.concatenation = tf.keras.layers.Concatenate(axis=1, name=\"concatenation\")\n",
    "        self.lr = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name=\"lr\",\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=config[\"l2_reg\"]))\n",
    "\n",
    "    def call(self, x, d):\n",
    "        x = unit_normalization(x)\n",
    "        return self.lr(self.concatenation([x, d]))\n",
    "\n",
    "def compute_loss(model, x, d, label):\n",
    "    prediction = model(x, d)\n",
    "    loss_sum = tf.negative(tf.add(tf.multiply(5, tf.multiply(label, tf.math.log(prediction))), \n",
    "                                  tf.multiply(tf.subtract(1., label), tf.math.log(tf.subtract(1., prediction)))))\n",
    "    return tf.reduce_mean(loss_sum)\n",
    "\n",
    "def calculate_auc(model, test_x, test_d, test_y, config):\n",
    "    AUC = tf.keras.metrics.AUC(num_thresholds=200)\n",
    "    AUC.reset_states()\n",
    "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
    "    pred = model(x, d)\n",
    "    AUC.update_state(y, pred)\n",
    "\n",
    "    return AUC.result().numpy()\n",
    "\n",
    "def calculate_ROC(model, test_x, test_d, test_y, config):\n",
    "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
    "    pred = model(x,d)\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, pred)\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "def calculate_ppv(model, test_x, test_d, test_y, config):\n",
    "    ppv = tf.keras.metrics.Precision(thresholds=0.8)\n",
    "    ppv.reset_states()\n",
    "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
    "    pred = model(x,d)\n",
    "    ppv.update_state(y, pred)\n",
    "    return ppv.result().numpy()\n",
    "\n",
    "def load_data(patient_record_path, demo_record_path, labels_path):\n",
    "    patient_record = pickle.load(open(patient_record_path, 'rb'))\n",
    "    demo_record = pickle.load(open(demo_record_path, 'rb'))\n",
    "    labels = pickle.load(open(labels_path, 'rb'))\n",
    "    \n",
    "    return patient_record, demo_record, labels\n",
    "\n",
    "def save_data(output_path, mydata):\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(mydata, f)\n",
    "\n",
    "def pad_matrix(records, demos, labels, config):\n",
    "    n_patients = len(records)\n",
    "    input_vocabsize = config[\"input_vocabsize\"]\n",
    "    demo_vocabsize = config[\"demo_vocabsize\"]\n",
    "\n",
    "    x = np.zeros((n_patients, input_vocabsize)).astype(np.float32) # sum of all visits of the patient\n",
    "    d = np.zeros((n_patients, demo_vocabsize)).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    \n",
    "    for idx, rec in enumerate(records):\n",
    "        for visit in rec:\n",
    "            x[idx, visit] += 1\n",
    "\n",
    "    x = np.clip(0, 1, x) # clip values bigger than 1.\n",
    "        \n",
    "    for idx, demo in enumerate(demos):\n",
    "        d[idx, int(demo[:-2])] = 1. # the last element of demos is age \n",
    "        d[idx, -1:] = demo[-1:]\n",
    "        \n",
    "    return x, d, y\n",
    "\n",
    "def shuffle_data(data1, data2, data3):\n",
    "    data1, data2, data3 = np.array(data1), np.array(data2), np.array(data3)\n",
    "    idx = np.arange(len(data1))\n",
    "    random.seed(1234)\n",
    "    random.shuffle(idx)\n",
    "\n",
    "    return data1[idx], data2[idx], data3[idx]\n",
    "\n",
    "def unit_normalization(myarray):\n",
    "    avg = tf.reshape(tf.math.reduce_mean(myarray, axis=-1), shape=(myarray.shape[0], 1))\n",
    "    std = tf.reshape(tf.math.reduce_std(myarray, axis=-1), shape=(myarray.shape[0], 1))\n",
    "    return tf.math.divide(tf.math.subtract(myarray, avg), std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lreg_kfold(output_path, patient_record_path, demo_record_path, labels_path, epochs, batch_size,\n",
    "                input_vocabsize, demo_vocabsize, l2_reg=0.001, learning_rate=0.001, k=5, times =5, notes=None):\n",
    "    \n",
    "    tf.random.set_seed(1234)\n",
    "    config = locals().copy()\n",
    "    for i in range(times):\n",
    "        version = i\n",
    "        print(\"load data...\")\n",
    "        recs, demos, labels = load_data(patient_record_path, demo_record_path, labels_path)\n",
    "\n",
    "        print(\"split the dataset into k-fold...\")\n",
    "        recs, demos, labels = shuffle_data(recs, demos, labels)\n",
    "        chunk_size = int(np.floor(len(labels) / k))\n",
    "        np.split(np.arange(len(labels)), [chunk_size*i for i in range(k)])\n",
    "        folds = np.tile(np.split(np.arange(len(labels)), [chunk_size*i for i in range(int(k))])[1:], 2)\n",
    "\n",
    "        k_fold_auc = []\n",
    "        k_fold_ppv = []\n",
    "        k_fold_tpr = []\n",
    "        mean_fpr = np.linspace(0,1,200)\n",
    "        k_fold_training_loss = []\n",
    "\n",
    "        for i in range(k):\n",
    "            train_x, test_x = recs[np.concatenate(folds[(i%k):(i%k)+k-1])], recs[folds[(i%k)+k]]\n",
    "            train_d, test_d = demos[np.concatenate(folds[(i%k):(i%k)+k-1])], demos[folds[(i%k)+k]]\n",
    "            train_y, test_y = labels[np.concatenate(folds[(i%k):(i%k)+k-1])], labels[folds[(i%k)+k]]\n",
    "\n",
    "            num_batches = int(np.ceil(float(len(train_x)) / float(batch_size)))\n",
    "            training_loss = []\n",
    "\n",
    "            print(\"build and initialize model for {k}th fold...\".format(k=i+1))\n",
    "            lr_model = LogisticRegression(config)\n",
    "\n",
    "            best_auc = 0\n",
    "            best_epoch = 0\n",
    "            best_model = None\n",
    "            print(\"start training...\")\n",
    "            for epoch in range(epochs):\n",
    "                loss_record = []\n",
    "                progbar = tf.keras.utils.Progbar(num_batches)\n",
    "\n",
    "                for t in random.sample(range(num_batches), num_batches):\n",
    "                    batch_x = train_x[t * batch_size:(t+1) * batch_size]\n",
    "                    batch_d = train_d[t * batch_size:(t+1) * batch_size]\n",
    "                    batch_y = train_y[t * batch_size:(t+1) * batch_size]\n",
    "\n",
    "                    x, d, y = pad_matrix(batch_x, batch_d, batch_y, config)\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        batch_cost = compute_loss(lr_model, x, d, y)\n",
    "                    gradients = tape.gradient(batch_cost, lr_model.trainable_variables)\n",
    "                    lr_model.optimizer.apply_gradients(zip(gradients, lr_model.trainable_variables))\n",
    "\n",
    "                    loss_record.append(batch_cost.numpy())\n",
    "                    progbar.add(1)\n",
    "\n",
    "                print('epoch:{e}, mean loss:{l:.6f}'.format(e=epoch+1, l=np.mean(loss_record)))\n",
    "                training_loss.append(np.mean(loss_record))\n",
    "                current_auc = calculate_auc(lr_model, test_x, test_d, test_y, config)\n",
    "                #print('epoch:{e}, current auc:{l:.6f}'.format(e=epoch+1, l=current_auc))\n",
    "                if current_auc > best_auc: \n",
    "                    best_auc = current_auc\n",
    "                    best_epoch = epoch+1\n",
    "                    best_model = lr_model.get_weights()\n",
    "\n",
    "            k_fold_training_loss.append(training_loss)\n",
    "            print(\"calculate AUC on the best model using the test set\")\n",
    "            lr_model.set_weights(best_model)\n",
    "            test_auc = calculate_auc(lr_model, test_x, test_d, test_y, config)\n",
    "            test_ppv = calculate_ppv(lr_model, test_x, test_d, test_y, config)\n",
    "            print(\"AUC of {k}th fold: {auc:.6f}\".format(k=i+1, auc=test_auc))\n",
    "            print(\"PPV of {k}th fold: {ppv:.6f}\".format(k=i+1, ppv=test_ppv))\n",
    "            k_fold_auc.append(test_auc)\n",
    "            k_fold_ppv.append(test_ppv)\n",
    "            fpr, tpr, thresholds = calculate_ROC(lr_model, test_x, test_d, test_y, config)\n",
    "            k_fold_tpr.append(np.interp(mean_fpr, fpr, tpr))\n",
    "\n",
    "        print(\"saving k-fold results...\")\n",
    "        mode_name = \"mhot\"\n",
    "        #np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_training_loss_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_training_loss)\n",
    "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_auc_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_auc)\n",
    "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_tpr_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_tpr)\n",
    "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_ppv_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_ppv)\n",
    "        np.save(os.path.join(output_path, \"LRS_{m}_e{e}_l{l}_model_ver{i}.npy\".format(m=mode_name, e=epochs, l=learning_rate, i=version)), lr_model.get_weights())\n",
    "\n",
    "    save_data(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_config.pkl\".format(k=k, m=mode_name, l=learning_rate)), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lreg_kfold('./ml_output', './patient_record.pkl', './demo_record.pkl','./labels.pkl', 20, 2,\n",
    "                1318, 4, l2_reg=0.001, learning_rate=0.001, k=5, times=5)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
